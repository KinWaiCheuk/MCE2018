{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from keras.layers import Input, Conv2D, Lambda, concatenate, Dense, Flatten,MaxPooling2D,Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = 5\n",
    "\n",
    "## making dictionary to find blacklist pair between train and test dataset\n",
    "bl_match = np.loadtxt('data/bl_matching.csv',dtype='str')\n",
    "dev2train={}\n",
    "dev2id={}\n",
    "train2dev={}\n",
    "train2id={}\n",
    "test2train={}\n",
    "train2test={}\n",
    "for iter, line in enumerate(bl_match):\n",
    "    line_s = line.split(',')\n",
    "    dev2train[line_s[1].split('_')[-1]]= line_s[3].split('_')[-1]\n",
    "    dev2id[line_s[1].split('_')[-1]]= line_s[0].split('_')[-1]\n",
    "    train2dev[line_s[3].split('_')[-1]]= line_s[1].split('_')[-1]\n",
    "    train2id[line_s[3].split('_')[-1]]= line_s[0].split('_')[-1]\n",
    "    test2train[line_s[2].split('_')[-1]]= line_s[3].split('_')[-1]\n",
    "    train2test[line_s[3].split('_')[-1]]= line_s[2].split('_')[-1]\n",
    "    \n",
    "def load_ivector(filename):\n",
    "    utt = np.loadtxt(filename,dtype='str',delimiter=',',skiprows=1,usecols=[0])\n",
    "    ivector = np.loadtxt(filename,dtype='float32',delimiter=',',skiprows=1,usecols=range(1,601))\n",
    "    spk_id = []\n",
    "    for iter in range(len(utt)):\n",
    "        spk_id = np.append(spk_id,utt[iter].split('_')[0])\n",
    "\n",
    "    return spk_id, utt, ivector\n",
    "\n",
    "def length_norm(mat):\n",
    "# length normalization (l2 norm)\n",
    "# input: mat = [utterances X vector dimension] ex) (float) 8631 X 600\n",
    "\n",
    "    norm_mat = []\n",
    "    for line in mat:\n",
    "        temp = line/np.math.sqrt(sum(np.power(line,2)))\n",
    "        norm_mat.append(temp)\n",
    "    norm_mat = np.array(norm_mat)\n",
    "    return norm_mat\n",
    "\n",
    "def make_spkvec(mat, spk_label):\n",
    "# calculating speaker mean vector\n",
    "# input: mat = [utterances X vector dimension] ex) (float) 8631 X 600\n",
    "#        spk_label = string vector ex) ['abce','cdgd']\n",
    "\n",
    "#     for iter in range(len(spk_label)):\n",
    "#         spk_label[iter] = spk_label[iter].split('_')[0]\n",
    "\n",
    "    spk_label, spk_index  = np.unique(spk_label,return_inverse=True)\n",
    "    spk_mean=[]\n",
    "    mat = np.array(mat)\n",
    "\n",
    "    # calculating speaker mean i-vector\n",
    "    for i, spk in enumerate(spk_label):\n",
    "        spk_mean.append(np.mean(mat[np.nonzero(spk_index==i)],axis=0))\n",
    "    spk_mean = length_norm(spk_mean)\n",
    "    return spk_mean, spk_label\n",
    "\n",
    "def calculate_EER(trials, scores):\n",
    "# calculating EER of Top-S detector\n",
    "# input: trials = boolean(or int) vector, 1: postive(blacklist) 0: negative(background)\n",
    "#        scores = float vector\n",
    "\n",
    "    # Calculating EER\n",
    "    fpr,tpr,threshold = roc_curve(trials,scores,pos_label=1)\n",
    "    fnr = 1-tpr\n",
    "    EER_threshold = threshold[np.argmin(abs(fnr-fpr))]\n",
    "    \n",
    "    # print EER_threshold\n",
    "    EER = fpr[np.argmin(np.absolute((fnr-fpr)))]\n",
    "    print(\"Top S detector EER is %0.2f%%\"% (EER*100))\n",
    "    return EER\n",
    "\n",
    "def get_trials_label_with_confusion(identified_label, groundtruth_label,dict4spk,is_trial ):\n",
    "# determine if the test utterance would make confusion error\n",
    "# input: identified_label = string vector, identified result of test utterance among multi-target from the detection system \n",
    "#        groundtruth_label = string vector, ground truth speaker labels of test utterances\n",
    "#        dict4spk = dictionary, convert label to target set, ex) train2dev convert train id to dev id\n",
    "\n",
    "    trials = np.zeros(len(identified_label))\n",
    "    for iter in range(0,len(groundtruth_label)):\n",
    "        enroll = identified_label[iter].split('_')[0]\n",
    "        test = groundtruth_label[iter].split('_')[0]\n",
    "        if is_trial[iter]:\n",
    "            if enroll == dict4spk[test]:\n",
    "                trials[iter]=1 # for Target trial (blacklist speaker)\n",
    "            else:\n",
    "                trials[iter]=-1 # for Target trial (backlist speaker), but fail on blacklist classifier\n",
    "                \n",
    "        else :\n",
    "            trials[iter]=0 # for non-target (non-blacklist speaker)\n",
    "    return trials\n",
    "\n",
    "\n",
    "def calculate_EER_with_confusion(scores,trials):\n",
    "# calculating EER of Top-1 detector\n",
    "# input: trials = boolean(or int) vector, 1: postive(blacklist) 0: negative(background) -1: confusion(blacklist)\n",
    "#        scores = float vector\n",
    "\n",
    "    # exclude confusion error (trials==-1)\n",
    "    scores_wo_confusion = scores[np.nonzero(trials!=-1)[0]]\n",
    "    trials_wo_confusion = trials[np.nonzero(trials!=-1)[0]]\n",
    "\n",
    "    # dev_trials contain labels of target. (target=1, non-target=0)\n",
    "    fpr,tpr,threshold = roc_curve(trials_wo_confusion,scores_wo_confusion,pos_label=1, drop_intermediate=False)\n",
    "    fnr = 1-tpr\n",
    "    EER_threshold = threshold[np.argmin(abs(fnr-fpr))]\n",
    "    \n",
    "    # EER withouth confusion error\n",
    "    EER = fpr[np.argmin(np.absolute((fnr-fpr)))]\n",
    "    \n",
    "    # Add confusion error to false negative rate(Miss rate)\n",
    "    total_negative = len(np.nonzero(np.array(trials_wo_confusion)==0)[0])\n",
    "    total_positive = len(np.nonzero(np.array(trials_wo_confusion)==1)[0])\n",
    "    fp= fpr*np.float(total_negative)  \n",
    "    fn= fnr*np.float(total_positive) \n",
    "    fn += len(np.nonzero(trials==-1)[0])\n",
    "    total_positive += len(np.nonzero(trials==-1)[0])\n",
    "    fpr= fp/total_negative\n",
    "    fnr= fn/total_positive\n",
    "\n",
    "    # EER with confusion Error\n",
    "    EER_threshold = threshold[np.argmin(abs(fnr-fpr))]\n",
    "    EER_fpr = fpr[np.argmin(np.absolute((fnr-fpr)))]\n",
    "    EER_fnr = fnr[np.argmin(np.absolute((fnr-fpr)))]\n",
    "    EER = 0.5 * (EER_fpr+EER_fnr)\n",
    "    \n",
    "    print(\"Top 1 detector EER is %0.2f%% (Total confusion error is %d)\"% ((EER*100), len(np.nonzero(trials==-1)[0])))\n",
    "    return EER,len(np.nonzero(trials==-1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dev set score using train set :\n",
      "Top S detector EER is 2.00%\n",
      "Top 1 detector EER is 12.26% (Total confusion error is 444)\n"
     ]
    }
   ],
   "source": [
    "## Loading i-vector\n",
    "# trn_bl_id, trn_bl_utt, trn_bl_ivector = load_ivector('data/trn_blacklist.csv')\n",
    "# trn_bg_id, trn_bg_utt, trn_bg_ivector = load_ivector('data/trn_background.csv')\n",
    "# dev_bl_id, dev_bl_utt, dev_bl_ivector = load_ivector('data/dev_blacklist.csv')\n",
    "# dev_bg_id, dev_bg_utt, dev_bg_ivector = load_ivector('data/dev_background.csv')\n",
    "\n",
    "\n",
    "trn_bl_ivector = pickle.load(open('./data/trn_bl_ivector','rb'))\n",
    "trn_bg_ivector = pickle.load(open('./data/trn_bg_ivector','rb'))\n",
    "dev_bl_ivector = pickle.load(open('./data/dev_bl_ivector','rb'))\n",
    "dev_bg_ivector = pickle.load(open('./data/dev_bg_ivector','rb'))\n",
    "trn_bl_id = pickle.load(open('./data/trn_bl_id','rb'))\n",
    "trn_bg_id = pickle.load(open('./data/trn_bg_id','rb'))\n",
    "dev_bl_id = pickle.load(open('./data/dev_bl_id','rb'))\n",
    "dev_bg_id = pickle.load(open('./data/dev_bg_id','rb'))\n",
    "trn_bl_utt = pickle.load(open('./data/trn_bl_utt','rb'))\n",
    "trn_bg_utt = pickle.load(open('./data/trn_bg_utt','rb'))\n",
    "dev_bl_utt = pickle.load(open('./data/dev_bl_utt','rb'))\n",
    "dev_bg_utt = pickle.load(open('./data/dev_bg_utt','rb'))\n",
    "tst_id = pickle.load(open('./data/tst_id','rb'))\n",
    "test_utt = pickle.load(open('./data/test_utt','rb'))\n",
    "tst_ivector = pickle.load(open('./data/tst_ivector','rb'))\n",
    "\n",
    "# Calculating speaker mean vector\n",
    "spk_mean, spk_mean_label = make_spkvec(trn_bl_ivector,trn_bl_id)\n",
    "\n",
    "#length normalization\n",
    "\n",
    "trn_bl_ivector = length_norm(trn_bl_ivector)\n",
    "trn_bg_ivector = length_norm(trn_bg_ivector)\n",
    "dev_bl_ivector = length_norm(dev_bl_ivector)\n",
    "dev_bg_ivector = length_norm(dev_bg_ivector)\n",
    "tst_ivector = length_norm(tst_ivector)\n",
    "\n",
    "filename = 'data/tst_evaluation_keys.csv'\n",
    "tst_info = np.loadtxt(filename,dtype='str',delimiter=',',skiprows=1,usecols=range(0,3))\n",
    "tst_trials = []\n",
    "tst_trials_label = []\n",
    "tst_ground_truth =[]\n",
    "for iter in range(len(tst_info)):\n",
    "    tst_trials_label.extend([tst_info[iter,0]])\n",
    "    if tst_info[iter,1]=='background':\n",
    "        tst_trials = np.append(tst_trials,0)\n",
    "        \n",
    "    else:\n",
    "        tst_trials = np.append(tst_trials,1)\n",
    "\n",
    "\n",
    "# making trials of Dev set\n",
    "dev_ivector = np.append(dev_bl_ivector, dev_bg_ivector,axis=0)\n",
    "dev_trials = np.append( np.ones([len(dev_bl_id), 1]), np.zeros([len(dev_bg_id), 1]))\n",
    "\n",
    "trn_ivector = np.append(trn_bl_ivector, trn_bg_ivector,axis=0)\n",
    "trn_trials = np.append( np.ones([len(trn_bl_ivector), 1]), np.zeros([len(trn_bg_ivector), 1]))\n",
    "\n",
    "print('\\nDev set score using train set :')\n",
    "# Cosine distance scoring\n",
    "scores = spk_mean.dot(dev_ivector.transpose())\n",
    "\n",
    "# Multi-target normalization\n",
    "blscores = spk_mean.dot(trn_bl_ivector.transpose())\n",
    "mnorm_mu = np.mean(blscores,axis=1)\n",
    "mnorm_std = np.std(blscores,axis=1)\n",
    "for iter in range(np.shape(scores)[1]):\n",
    "    scores[:,iter]= (scores[:,iter] - mnorm_mu) / mnorm_std\n",
    "dev_scores = np.max(scores,axis=0)\n",
    "\n",
    "# Top-S detector EER\n",
    "dev_EER = calculate_EER(dev_trials, dev_scores)\n",
    "\n",
    "#divide trial label into target and non-target, plus confusion error(blacklist, fail at blacklist detector)\n",
    "dev_identified_label = spk_mean_label[np.argmax(scores,axis=0)]\n",
    "dev_trials_label = np.append( dev_bl_id,dev_bg_id)\n",
    "dev_trials_utt_label = np.append( dev_bl_utt,dev_bg_utt)\n",
    "\n",
    "# Top-1 detector EER\n",
    "dev_trials_confusion = get_trials_label_with_confusion(dev_identified_label, dev_trials_label, dev2train, dev_trials )\n",
    "dev_EER_confusion,trials = calculate_EER_with_confusion(dev_scores,dev_trials_confusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "              solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_set = sorted(set(trn_bl_id))\n",
    "\n",
    "id2int = {}\n",
    "for i,spk_id in enumerate(id_set):\n",
    "    id2int[spk_id] = i\n",
    "\n",
    "int2id = {v: k for k, v in id2int.items()}\n",
    "\n",
    "\n",
    "## Generating subclass for Testing\n",
    "\n",
    "classes = 3631\n",
    "\n",
    "## Generating (Anchor, Positive, Negative) pairs for One-shot-learning\n",
    "all_data = np.column_stack((trn_bl_ivector.reshape(classes,3,600),dev_bl_ivector.reshape(classes,1,600)))\n",
    "\n",
    "label_ls = []\n",
    "for i in range(3631):\n",
    "    for label in range(4):\n",
    "        label_ls.append(i)\n",
    "label_array = np.array(label_ls)\n",
    "\n",
    "dev2trn_bl_id = []\n",
    "for i in dev_bl_id:\n",
    "    dev2trn_bl_id.append(dev2train[i])\n",
    "dev2trn_bl_id = np.array(dev2trn_bl_id)\n",
    "# if i == 0:\n",
    "    # None\n",
    "# else:\n",
    "    # model.load_weights(foldername+filename+'.hdf5')\n",
    "# X_train, X_test = triplet_generation(all_data,neg_class_num=50)\n",
    "#Use trained model to predict\n",
    "trained_model_task1a = LinearDiscriminantAnalysis()\n",
    "trained_model_task1a.fit(trn_ivector,trn_trials)\n",
    "\n",
    "trained_model_task1b = LinearDiscriminantAnalysis()\n",
    "trained_model_task1b.fit(np.append(trn_ivector,dev_ivector, axis=0),np.append(trn_trials,dev_trials))\n",
    "\n",
    "trained_model_task2a = LinearDiscriminantAnalysis()\n",
    "trained_model_task2a.fit(trn_bl_ivector,trn_bl_id)\n",
    "\n",
    "trained_model_task2b = LinearDiscriminantAnalysis()\n",
    "trained_model_task2b.fit(np.append(trn_bl_ivector,dev_bl_ivector, axis=0),np.append(trn_bl_id,dev2trn_bl_id))\n",
    "# trained_model.load_weights(filename+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top S detector EER is 0.46%\n",
      "Top 1 detector EER is 8.62% (Total confusion error is 309)\n"
     ]
    }
   ],
   "source": [
    "scores = trained_model_task1a.predict(dev_ivector)\n",
    "dev_EER = calculate_EER(dev_trials, scores)\n",
    "\n",
    "##############################1st Report#####################################\n",
    "# Cosine distance scoring\n",
    "dev_identified_label = trained_model_task2a.predict(dev_ivector)\n",
    "\n",
    "#divide trial label into target and non-target, plus confusion error(blacklist, fail at blacklist detector)\n",
    "# dev_identified_label = spk_mean_label[np.argmax(scores,axis=0)]\n",
    "# dev_trials_label = np.append( dev_bl_id,dev_bg_id)\n",
    "# dev_trials_utt_label = np.append( dev_bl_utt,dev_bg_utt)\n",
    "\n",
    "# Top-1 detector EER\n",
    "dev_trials_confusion = get_trials_label_with_confusion(dev_identified_label, dev_trials_label, dev2train, dev_trials )\n",
    "dev_EER_confusion,confu_num1 = calculate_EER_with_confusion(dev_scores,dev_trials_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score using train set:\n",
      "Top S detector EER is 25.07%\n",
      "Top 1 detector EER is 18.14% (Total confusion error is 367)\n",
      "\n",
      "Test set score using train + dev set:\n",
      "Top S detector EER is 23.67%\n",
      "Top 1 detector EER is 15.62% (Total confusion error is 230)\n"
     ]
    }
   ],
   "source": [
    "# transformed_trn_bl_ivector = trained_model.predict(trn_bl_ivector)\n",
    "# transformed_dev_ivector = trained_model.predict(dev_ivector)\n",
    "# transformed_trn_ivector = trained_model.predict(trn_ivector)\n",
    "# transformed_tst_ivector = trained_model.predict(tst_ivector)\n",
    "# transformed_dev_bl_ivector = trained_model.predict(dev_bl_ivector)\n",
    "# transformed_spk_mean, transformed_spk_mean_label = make_spkvec(transformed_trn_bl_ivector,trn_bl_id)\n",
    "print('\\nTest set score using train set:')\n",
    "scores = trained_model_task1a.predict(tst_ivector)\n",
    "dev_EER = calculate_EER(tst_trials, scores)\n",
    "\n",
    "##############################1st Report#####################################\n",
    "# Cosine distance scoring\n",
    "tst_identified_label = trained_model_task2a.predict(tst_ivector)\n",
    "\n",
    "#divide trial label into target and non-target, plus confusion error(blacklist, fail at blacklist detector)\n",
    "# dev_identified_label = spk_mean_label[np.argmax(scores,axis=0)]\n",
    "# dev_trials_label = np.append( dev_bl_id,dev_bg_id)\n",
    "# dev_trials_utt_label = np.append( dev_bl_utt,dev_bg_utt)\n",
    "\n",
    "# Top-1 detector EER\n",
    "tst_trials_confusion = get_trials_label_with_confusion(tst_identified_label, tst_trials_label, test2train,tst_trials )\n",
    "tst_EER_confusion,confu_num3 = calculate_EER_with_confusion(scores,tst_trials_confusion)  \n",
    "\n",
    "\n",
    "################################2nd Report######################################\n",
    "\n",
    "\n",
    "\n",
    "################################3rd Report #########################################\n",
    "print('\\nTest set score using train + dev set:')\n",
    "# get dev set id consistent with Train set\n",
    "scores = trained_model_task1b.predict(tst_ivector)\n",
    "dev_EER = calculate_EER(tst_trials, scores)\n",
    "\n",
    "##############################1st Report#####################################\n",
    "# Cosine distance scoring\n",
    "tst_identified_label = trained_model_task2b.predict(tst_ivector)\n",
    "\n",
    "#divide trial label into target and non-target, plus confusion error(blacklist, fail at blacklist detector)\n",
    "# dev_identified_label = spk_mean_label[np.argmax(scores,axis=0)]\n",
    "# dev_trials_label = np.append( dev_bl_id,dev_bg_id)\n",
    "# dev_trials_utt_label = np.append( dev_bl_utt,dev_bg_utt)\n",
    "\n",
    "# Top-1 detector EER\n",
    "tst_trials_confusion = get_trials_label_with_confusion(tst_identified_label, tst_trials_label, test2train,tst_trials )\n",
    "tst_EER_confusion,confu_num3 = calculate_EER_with_confusion(scores,tst_trials_confusion)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16017,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_trials.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16017,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
