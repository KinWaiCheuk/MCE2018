{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, concatenate, Dense, Flatten,MaxPooling2D,Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib \n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "from itertools import permutations\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restricting GPU memory\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "gpu = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_optim = Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "def generate_triplet(x,y,testsize=0.3,ap_pairs=10,an_pairs=10):\n",
    "    data_xy = tuple([x,y])\n",
    "\n",
    "    trainsize = 1-testsize\n",
    "\n",
    "    triplet_train_pairs = []\n",
    "    triplet_test_pairs = []\n",
    "    for data_class in sorted(set(data_xy[1])):\n",
    "\n",
    "        same_class_idx = np.where((data_xy[1] == data_class))[0]\n",
    "        diff_class_idx = np.where(data_xy[1] != data_class)[0]       \n",
    "        if same_class_idx.shape[0] < 15:\n",
    "            same_class_sampleer_idx = random.choice(range(len(same_class_idx)))\n",
    "            A_P_pairs = random.sample(list(permutations(same_class_idx,2)),k=ap_pairs) #Generating Anchor-Positive pairs\n",
    "        else:\n",
    "            same_class_sampleer_idx = random.choice(range(len(same_class_idx)-21))\n",
    "            A_P_pairs = random.sample(list(permutations(same_class_idx[same_class_sampleer_idx:same_class_sampleer_idx+21],2)),k=ap_pairs) #Generating Anchor-Positive pairs\n",
    "        Neg_idx = random.sample(list(diff_class_idx),k=an_pairs)\n",
    "        \n",
    "\n",
    "        #train\n",
    "        A_P_len = len(A_P_pairs)\n",
    "        Neg_len = len(Neg_idx)\n",
    "        for ap in A_P_pairs[:int(A_P_len*trainsize)]:\n",
    "            Anchor = data_xy[0][ap[0]]\n",
    "            Positive = data_xy[0][ap[1]]\n",
    "            for n in Neg_idx:\n",
    "                Negative = data_xy[0][n]\n",
    "                triplet_train_pairs.append([Anchor,Positive,Negative])               \n",
    "        #test\n",
    "        for ap in A_P_pairs[int(A_P_len*trainsize):]:\n",
    "            Anchor = data_xy[0][ap[0]]\n",
    "            Positive = data_xy[0][ap[1]]\n",
    "            for n in Neg_idx:\n",
    "                Negative = data_xy[0][n]\n",
    "                triplet_test_pairs.append([Anchor,Positive,Negative])    \n",
    "                \n",
    "    return np.array(triplet_train_pairs), np.array(triplet_test_pairs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# X_train = triplet_generation(trn_bl_ivector_10Classes.reshape(100,3,600), neg_class_num=20)\n",
    "\n",
    "# X_train, X_test = triplet_generation(trn_bl_ivector.reshape(classes,3,600),neg_class_num=30)\n",
    "\n",
    "## Triplet NN\n",
    "\n",
    "def triplet_loss(y_true, y_pred, alpha = 5):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss function\n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor data\n",
    "            positive -- the encodings for the positive data (similar to anchor)\n",
    "            negative -- the encodings for the negative data (different from anchor)\n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    total_lenght = y_pred.shape.as_list()[-1]\n",
    "    anchor = y_pred[:,0:int(total_lenght*1/3)]\n",
    "    positive = y_pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]\n",
    "    negative = y_pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
    "\n",
    "    # distance between the anchor and the positive\n",
    "    pos_dist = K.sum(K.square(anchor-positive),axis=1)\n",
    "\n",
    "    # distance between the anchor and the negative\n",
    "    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n",
    "\n",
    "    # compute loss\n",
    "    basic_loss = pos_dist-neg_dist+alpha\n",
    "    loss = K.maximum(basic_loss,0.0)\n",
    " \n",
    "    return loss\n",
    "def create_base_network(in_dims, out_dims):\n",
    "    \"\"\"\n",
    "    Base network to be shared.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(600, input_shape=(in_dims,),activation='relu'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "anchor_input (InputLayer)       (None, 600)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive_input (InputLayer)     (None, 600)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negative_input (InputLayer)     (None, 600)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 600)          360600      anchor_input[0][0]               \n",
      "                                                                 positive_input[0][0]             \n",
      "                                                                 negative_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1800)         0           sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "                                                                 sequential_1[3][0]               \n",
      "==================================================================================================\n",
      "Total params: 360,600\n",
      "Trainable params: 360,600\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_TNN():\n",
    "    anchor_input = Input((600, ), name='anchor_input')\n",
    "    positive_input = Input((600, ), name='positive_input')\n",
    "    negative_input = Input((600, ), name='negative_input')\n",
    "\n",
    "    # Shared embedding layer for positive and negative items\n",
    "    Shared_DNN = create_base_network(600,600)\n",
    "\n",
    "    encoded_anchor = Shared_DNN(anchor_input)\n",
    "    encoded_positive = Shared_DNN(positive_input)\n",
    "    encoded_negative = Shared_DNN(negative_input)\n",
    "\n",
    "    merged_vector = concatenate([encoded_anchor, encoded_positive, encoded_negative], axis=-1)\n",
    "    model = Model(inputs=[anchor_input,positive_input, negative_input], outputs=merged_vector)\n",
    "    model.summary()\n",
    "    \n",
    "    # after training the model, we only need anchor_input and its output encoded_anchor\n",
    "    return model, anchor_input, encoded_anchor\n",
    "\n",
    "model, anchor_input, encoded_anchor = create_TNN()\n",
    "\n",
    "## making dictionary to find blacklist pair between train and test dataset\n",
    "# bl_match = np.loadtxt('data/bl_matching_dev.csv',dtype='string')\n",
    "bl_match = np.loadtxt('data/bl_matching.csv',dtype='str')\n",
    "dev2train={}\n",
    "dev2id={}\n",
    "train2dev={}\n",
    "train2id={}\n",
    "test2train={}\n",
    "train2test={}\n",
    "for iter, line in enumerate(bl_match):\n",
    "    line_s = line.split(',')\n",
    "    dev2train[line_s[1].split('_')[-1]]= line_s[3].split('_')[-1]\n",
    "    dev2id[line_s[1].split('_')[-1]]= line_s[0].split('_')[-1]\n",
    "    train2dev[line_s[3].split('_')[-1]]= line_s[1].split('_')[-1]\n",
    "    train2id[line_s[3].split('_')[-1]]= line_s[0].split('_')[-1]\n",
    "    test2train[line_s[2].split('_')[-1]]= line_s[3].split('_')[-1]\n",
    "    train2test[line_s[3].split('_')[-1]]= line_s[2].split('_')[-1]\n",
    "\n",
    "# import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "\n",
    "\n",
    "\n",
    "## helper functions\n",
    "\n",
    "def load_ivector(filename):\n",
    "    utt = np.loadtxt(filename,dtype='str',delimiter=',',skiprows=1,usecols=[0])\n",
    "    ivector = np.loadtxt(filename,dtype='float32',delimiter=',',skiprows=1,usecols=range(1,601))\n",
    "    spk_id = []\n",
    "    for iter in range(len(utt)):\n",
    "        spk_id = np.append(spk_id,utt[iter].split('_')[0])\n",
    "\n",
    "    return spk_id, utt, ivector\n",
    "\n",
    "def length_norm(mat):\n",
    "# length normalization (l2 norm)\n",
    "# input: mat = [utterances X vector dimension] ex) (float) 8631 X 600\n",
    "\n",
    "    norm_mat = []\n",
    "    for line in mat:\n",
    "        temp = line/np.math.sqrt(sum(np.power(line,2)))\n",
    "        norm_mat.append(temp)\n",
    "    norm_mat = np.array(norm_mat)\n",
    "    return norm_mat\n",
    "\n",
    "def make_spkvec(mat, spk_label):\n",
    "# calculating speaker mean vector\n",
    "# input: mat = [utterances X vector dimension] ex) (float) 8631 X 600\n",
    "#        spk_label = string vector ex) ['abce','cdgd']\n",
    "\n",
    "#     for iter in range(len(spk_label)):\n",
    "#         spk_label[iter] = spk_label[iter].split('_')[0]\n",
    "\n",
    "    spk_label, spk_index  = np.unique(spk_label,return_inverse=True)\n",
    "    spk_mean=[]\n",
    "    mat = np.array(mat)\n",
    "\n",
    "    # calculating speaker mean i-vector\n",
    "    for i, spk in enumerate(spk_label):\n",
    "        spk_mean.append(np.mean(mat[np.nonzero(spk_index==i)],axis=0))\n",
    "    spk_mean = length_norm(spk_mean)\n",
    "    return spk_mean, spk_label\n",
    "\n",
    "def calculate_EER(trials, scores):\n",
    "# calculating EER of Top-S detector\n",
    "# input: trials = boolean(or int) vector, 1: postive(blacklist) 0: negative(background)\n",
    "#        scores = float vector\n",
    "\n",
    "    # Calculating EER\n",
    "    fpr,tpr,threshold = roc_curve(trials,scores,pos_label=1)\n",
    "    fnr = 1-tpr\n",
    "    EER_threshold = threshold[np.argmin(abs(fnr-fpr))]\n",
    "    \n",
    "    # print EER_threshold\n",
    "    EER_fpr = fpr[np.argmin(np.absolute((fnr-fpr)))]\n",
    "    EER_fnr = fnr[np.argmin(np.absolute((fnr-fpr)))]\n",
    "    EER = 0.5 * (EER_fpr+EER_fnr)\n",
    "    \n",
    "    print(\"Top S detector EER is %0.2f%%\"% (EER*100))\n",
    "    return EER\n",
    "\n",
    "def get_trials_label_with_confusion(identified_label, groundtruth_label,dict4spk,is_trial ):\n",
    "# determine if the test utterance would make confusion error\n",
    "# input: identified_label = string vector, identified result of test utterance among multi-target from the detection system \n",
    "#        groundtruth_label = string vector, ground truth speaker labels of test utterances\n",
    "#        dict4spk = dictionary, convert label to target set, ex) train2dev convert train id to dev id\n",
    "\n",
    "    trials = np.zeros(len(identified_label))\n",
    "    for iter in range(0,len(groundtruth_label)):\n",
    "        enroll = identified_label[iter].split('_')[0]\n",
    "        test = groundtruth_label[iter].split('_')[0]\n",
    "        if is_trial[iter]:\n",
    "            if enroll == dict4spk[test]:\n",
    "                trials[iter]=1 # for Target trial (blacklist speaker)\n",
    "            else:\n",
    "                trials[iter]=-1 # for Target trial (backlist speaker), but fail on blacklist classifier\n",
    "                \n",
    "        else :\n",
    "            trials[iter]=0 # for non-target (non-blacklist speaker)\n",
    "    return trials\n",
    "\n",
    "\n",
    "def calculate_EER_with_confusion(scores,trials):\n",
    "# calculating EER of Top-1 detector\n",
    "# input: trials = boolean(or int) vector, 1: postive(blacklist) 0: negative(background) -1: confusion(blacklist)\n",
    "#        scores = float vector\n",
    "\n",
    "    # exclude confusion error (trials==-1)\n",
    "    scores_wo_confusion = scores[np.nonzero(trials!=-1)[0]]\n",
    "    trials_wo_confusion = trials[np.nonzero(trials!=-1)[0]]\n",
    "\n",
    "    # dev_trials contain labels of target. (target=1, non-target=0)\n",
    "    fpr,tpr,threshold = roc_curve(trials_wo_confusion,scores_wo_confusion,pos_label=1, drop_intermediate=False)\n",
    "    fnr = 1-tpr\n",
    "    EER_threshold = threshold[np.argmin(abs(fnr-fpr))]\n",
    "    \n",
    "    # EER withouth confusion error\n",
    "    EER = fpr[np.argmin(np.absolute((fnr-fpr)))]\n",
    "    \n",
    "    # Add confusion error to false negative rate(Miss rate)\n",
    "    total_negative = len(np.nonzero(np.array(trials_wo_confusion)==0)[0])\n",
    "    total_positive = len(np.nonzero(np.array(trials_wo_confusion)==1)[0])\n",
    "    fp= fpr*np.float(total_negative)  \n",
    "    fn= fnr*np.float(total_positive) \n",
    "    fn += len(np.nonzero(trials==-1)[0])\n",
    "    total_positive += len(np.nonzero(trials==-1)[0])\n",
    "    fpr= fp/total_negative\n",
    "    fnr= fn/total_positive\n",
    "\n",
    "    # EER with confusion Error\n",
    "    EER_threshold = threshold[np.argmin(abs(fnr-fpr))]\n",
    "    EER_fpr = fpr[np.argmin(np.absolute((fnr-fpr)))]\n",
    "    EER_fnr = fnr[np.argmin(np.absolute((fnr-fpr)))]\n",
    "    EER = 0.5 * (EER_fpr+EER_fnr)\n",
    "    \n",
    "    print(\"Top 1 detector EER is %0.2f%% (Total confusion error is %d)\"% ((EER*100), len(np.nonzero(trials==-1)[0])))\n",
    "    return EER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ivectors():\n",
    "    # Loading i-vectors for train set, dev set and test set\n",
    "    trn_bl_ivector = pickle.load(open('./data/trn_bl_ivector','rb'))\n",
    "    trn_bg_ivector = pickle.load(open('./data/trn_bg_ivector','rb'))\n",
    "    dev_bl_ivector = pickle.load(open('./data/dev_bl_ivector','rb'))\n",
    "    dev_bg_ivector = pickle.load(open('./data/dev_bg_ivector','rb'))\n",
    "    tst_ivector = pickle.load(open('./data/tst_ivector','rb'))\n",
    "    \n",
    "    #length normalization\n",
    "    trn_bl_ivector = length_norm(trn_bl_ivector)\n",
    "    trn_bg_ivector = length_norm(trn_bg_ivector)\n",
    "    dev_bl_ivector = length_norm(dev_bl_ivector)\n",
    "    dev_bg_ivector = length_norm(dev_bg_ivector)\n",
    "    tst_ivector = length_norm(tst_ivector)\n",
    "    \n",
    "    return trn_bl_ivector, trn_bg_ivector, dev_bl_ivector, dev_bg_ivector, tst_ivector\n",
    "\n",
    "def get_spk_ids():\n",
    "    # Loading speaker ID, for task 2\n",
    "    trn_bl_id = pickle.load(open('./data/trn_bl_id','rb'))\n",
    "    trn_bg_id = pickle.load(open('./data/trn_bg_id','rb'))\n",
    "    dev_bl_id = pickle.load(open('./data/dev_bl_id','rb'))\n",
    "    dev_bg_id = pickle.load(open('./data/dev_bg_id','rb'))\n",
    "    tst_id = pickle.load(open('./data/tst_id','rb'))\n",
    "    \n",
    "    return trn_bl_id, trn_bg_id, dev_bl_id, dev_bg_id, tst_id\n",
    "\n",
    "def get_spk_utt():\n",
    "    # Loading speaker utt\n",
    "    trn_bl_utt = pickle.load(open('./data/trn_bl_utt','rb'))\n",
    "    trn_bg_utt = pickle.load(open('./data/trn_bg_utt','rb'))\n",
    "    dev_bl_utt = pickle.load(open('./data/dev_bl_utt','rb'))\n",
    "    dev_bg_utt = pickle.load(open('./data/dev_bg_utt','rb'))\n",
    "    test_utt = pickle.load(open('./data/test_utt','rb'))\n",
    "\n",
    "    return trn_bl_utt, trn_bg_utt, dev_bl_utt, dev_bg_utt, test_utt\n",
    "\n",
    "def get_tst_trials():\n",
    "    # creating test labels, for task 1\n",
    "    filename = 'data/tst_evaluation_keys.csv'\n",
    "    tst_info = np.loadtxt(filename,dtype='str',delimiter=',',skiprows=1,usecols=range(0,3))\n",
    "    tst_trials = []\n",
    "    tst_trials_label = []\n",
    "    tst_ground_truth =[]\n",
    "    for iter in range(len(tst_info)):\n",
    "        tst_trials_label.extend([tst_info[iter,0]])\n",
    "        if tst_info[iter,1]=='background':\n",
    "            tst_trials = np.append(tst_trials,0)\n",
    "\n",
    "        else:\n",
    "            tst_trials = np.append(tst_trials,1)\n",
    "\n",
    "    return tst_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting i-vectors\n",
    "trn_bl_ivector, trn_bg_ivector, dev_bl_ivector, dev_bg_ivector, tst_ivector = get_ivectors()\n",
    "\n",
    "# Loading labels for task 2\n",
    "trn_bl_id, trn_bg_id, dev_bl_id, dev_bg_id, tst_id = get_spk_ids()\n",
    "\n",
    "# Making labels for task 1\n",
    "trn_ivector = np.append(trn_bl_ivector, trn_bg_ivector,axis=0) # combining bg and bl speakers into a single vector\n",
    "trn_trials = np.append( np.ones([len(trn_bl_ivector), 1]), np.zeros([len(trn_bg_ivector), 1]))\n",
    "dev_ivector = np.append(dev_bl_ivector, dev_bg_ivector,axis=0) # combining bg and bl\n",
    "dev_trials = np.append( np.ones([len(dev_bl_id), 1]), np.zeros([len(dev_bg_id), 1]))\n",
    "tst_trials = get_tst_trials()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dev set score using train set :\n",
      "Top S detector EER is 0.94%\n"
     ]
    }
   ],
   "source": [
    "total_loss_history = []\n",
    "total_valloss_history = []\n",
    "err_ls = []\n",
    "tst_EER1_ls = []\n",
    "tst_EER2_ls = []\n",
    "\n",
    "trained_model = Model(inputs=anchor_input, outputs=encoded_anchor)\n",
    "transformed_trn_bl_ivector = trained_model.predict(trn_bl_ivector)\n",
    "transformed_dev_ivector = trained_model.predict(dev_ivector)\n",
    "transformed_trn_ivector = trained_model.predict(trn_ivector)\n",
    "transformed_tst_ivector = trained_model.predict(tst_ivector)\n",
    "transformed_dev_bl_ivector = trained_model.predict(dev_bl_ivector)\n",
    "transformed_spk_mean, transformed_spk_mean_label = make_spkvec(transformed_trn_bl_ivector,trn_bl_id)\n",
    "\n",
    "#################Report first score###################    \n",
    "scores = transformed_spk_mean.dot(transformed_dev_ivector.transpose())\n",
    "\n",
    "# Multi-target normalization\n",
    "blscores = transformed_spk_mean.dot(transformed_trn_bl_ivector.transpose())\n",
    "mnorm_mu = np.mean(blscores,axis=1)\n",
    "mnorm_std = np.std(blscores,axis=1)\n",
    "for iter in range(np.shape(scores)[1]):\n",
    "    scores[:,iter]= (scores[:,iter] - mnorm_mu) / mnorm_std\n",
    "dev_scores = np.max(scores,axis=0)\n",
    "\n",
    "# Top-S detector EER\n",
    "print('\\nDev set score using train set :')\n",
    "err = calculate_EER(dev_trials, dev_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score using train set:\n",
      "Top S detector EER is 8.49%\n",
      "\n",
      "Test set score using train + dev set:\n",
      "Top S detector EER is 9.06%\n"
     ]
    }
   ],
   "source": [
    "#################Report second score#####################  \n",
    "print('\\nTest set score using train set:')  \n",
    "scores = transformed_spk_mean.dot(transformed_tst_ivector.transpose())\n",
    "# Multi-target normalization\n",
    "blscores = transformed_spk_mean.dot(transformed_trn_bl_ivector.transpose())\n",
    "mnorm_mu = np.mean(blscores,axis=1)\n",
    "mnorm_std = np.std(blscores,axis=1)\n",
    "for iter in range(np.shape(scores)[1]):\n",
    "    scores[:,iter]= (scores[:,iter] - mnorm_mu) / mnorm_std\n",
    "tst_scores = np.max(scores,axis=0)\n",
    "tst_EER1 = calculate_EER(tst_trials, tst_scores)    \n",
    "#################Report thrid score######################    \n",
    "print('\\nTest set score using train + dev set:')\n",
    "# get dev set id consistent with Train set\n",
    "dev_bl_id_along_trnset = []\n",
    "for iter in range(len(dev_bl_id)):\n",
    "    dev_bl_id_along_trnset.extend([dev2train[dev_bl_id[iter]]])\n",
    "\n",
    "# Calculating speaker mean vector\n",
    "# spk_mean, spk_mean_label = make_spkvec(np.append(trn_bl_ivector,dev_bl_ivector,0),np.append(trn_bl_id,dev_bl_id_along_trnset))\n",
    "transformed_spk_mean, transformed_spk_mean_label = make_spkvec(np.append(transformed_trn_ivector,transformed_dev_bl_ivector,0),np.append(trn_bl_id,dev_bl_id_along_trnset))\n",
    "\n",
    "#Cosine distance scoring on Test set\n",
    "scores = transformed_spk_mean.dot(transformed_tst_ivector.transpose())\n",
    "# # tst_scores = np.max(scores,axis=0)\n",
    "\n",
    "\n",
    "# # Multi-target normalization\n",
    "blscores = transformed_spk_mean.dot(np.append(transformed_trn_ivector.transpose(),transformed_dev_bl_ivector.transpose(),axis=1))\n",
    "mnorm_mu = np.mean(blscores,axis=1)\n",
    "mnorm_std = np.std(blscores,axis=1)\n",
    "for iter in range(np.shape(scores)[1]):\n",
    "    scores[:,iter]= (scores[:,iter] - mnorm_mu) / mnorm_std\n",
    "tst_scores = np.max(scores,axis=0)\n",
    "\n",
    "# top-S detector EER\n",
    "\n",
    "tst_EER2 = calculate_EER(tst_trials, tst_scores)\n",
    "\n",
    "err_ls.append(err*100)\n",
    "tst_EER1_ls.append(tst_EER1*100)\n",
    "tst_EER2_ls.append(tst_EER2*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
